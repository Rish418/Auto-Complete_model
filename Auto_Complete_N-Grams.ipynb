{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:29:47.273088Z",
     "iopub.status.busy": "2021-10-12T07:29:47.272781Z",
     "iopub.status.idle": "2021-10-12T07:29:47.281175Z",
     "shell.execute_reply": "2021-10-12T07:29:47.279949Z",
     "shell.execute_reply.started": "2021-10-12T07:29:47.273058Z"
    }
   },
   "source": [
    "#  Auto Complete using N-Grams Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:03:00.707972Z",
     "iopub.status.busy": "2021-10-12T07:03:00.707292Z",
     "iopub.status.idle": "2021-10-12T07:03:23.260086Z",
     "shell.execute_reply": "2021-10-12T07:03:23.259193Z",
     "shell.execute_reply.started": "2021-10-12T07:03:00.707934Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Basic File Paths\n",
    "data_dir = \"../input/tweets-blogs-news-swiftkey-dataset-4million/final/en_US\"\n",
    "file_path = data_dir + \"/en_US.twitter.txt\"\n",
    "\n",
    "## nltk settings\n",
    "nltk.data.path.append(data_dir)\n",
    "nltk.download('punkt')\n",
    "\n",
    "## Opening the File in read mode (\"r\")\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:03:23.262132Z",
     "iopub.status.busy": "2021-10-12T07:03:23.261808Z",
     "iopub.status.idle": "2021-10-12T07:03:23.268880Z",
     "shell.execute_reply": "2021-10-12T07:03:23.267912Z",
     "shell.execute_reply.started": "2021-10-12T07:03:23.262101Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_pipeline(data) -> 'list':\n",
    "\n",
    "    # Split by newline character\n",
    "    sentences = data.split('\\n')\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    # Drop Empty Sentences\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    # Empty List to hold Tokenized Sentences\n",
    "    tokenized = []\n",
    "    \n",
    "    # Iterate through sentences\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Convert to a list of words\n",
    "        token = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # Append to list\n",
    "        tokenized.append(token)\n",
    "        \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:03:23.270749Z",
     "iopub.status.busy": "2021-10-12T07:03:23.270433Z",
     "iopub.status.idle": "2021-10-12T07:14:05.715512Z",
     "shell.execute_reply": "2021-10-12T07:14:05.714566Z",
     "shell.execute_reply.started": "2021-10-12T07:03:23.270713Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = preprocess_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:14:05.717603Z",
     "iopub.status.busy": "2021-10-12T07:14:05.717336Z",
     "iopub.status.idle": "2021-10-12T07:14:08.892099Z",
     "shell.execute_reply": "2021-10-12T07:14:08.891133Z",
     "shell.execute_reply.started": "2021-10-12T07:14:05.717572Z"
    }
   },
   "outputs": [],
   "source": [
    "## Obtain Train and Test Split \n",
    "train, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "## Obtain Train and Validation Split \n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:14:09.150955Z",
     "iopub.status.busy": "2021-10-12T07:14:09.150647Z",
     "iopub.status.idle": "2021-10-12T07:14:09.157497Z",
     "shell.execute_reply": "2021-10-12T07:14:09.156150Z",
     "shell.execute_reply.started": "2021-10-12T07:14:09.150923Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_the_words(sentences) -> 'dict':\n",
    "    \n",
    "  # Creating a Dictionary of counts\n",
    "  word_counts = {}\n",
    "\n",
    "  # Iterating over sentences\n",
    "  for sentence in sentences:\n",
    "    \n",
    "    # Iterating over Tokens\n",
    "    for token in sentence:\n",
    "    \n",
    "      # Add count for new word\n",
    "      if token not in word_counts.keys():\n",
    "        word_counts[token] = 1\n",
    "        \n",
    "      # Increase count by one\n",
    "      else:\n",
    "        word_counts[token] += 1\n",
    "        \n",
    "  return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:14:09.159165Z",
     "iopub.status.busy": "2021-10-12T07:14:09.158898Z",
     "iopub.status.idle": "2021-10-12T07:14:09.177555Z",
     "shell.execute_reply": "2021-10-12T07:14:09.176614Z",
     "shell.execute_reply.started": "2021-10-12T07:14:09.159136Z"
    }
   },
   "outputs": [],
   "source": [
    "def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n",
    "\n",
    "  # Empty list for closed vocabulary\n",
    "  closed_vocabulary = []\n",
    "\n",
    "  # Obtain frequency dictionary using previously defined function\n",
    "  words_count = count_the_words(tokenized_sentences)\n",
    "    \n",
    "  # Iterate over words and counts \n",
    "  for word, count in words_count.items():\n",
    "    \n",
    "    # Append if it's more(or equal) to the threshold \n",
    "    if count >= count_threshold :\n",
    "      closed_vocabulary.append(word)\n",
    "\n",
    "  return closed_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:14:09.179208Z",
     "iopub.status.busy": "2021-10-12T07:14:09.178954Z",
     "iopub.status.idle": "2021-10-12T07:14:09.189905Z",
     "shell.execute_reply": "2021-10-12T07:14:09.188999Z",
     "shell.execute_reply.started": "2021-10-12T07:14:09.179181Z"
    }
   },
   "outputs": [],
   "source": [
    "def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n",
    "\n",
    "  # Convert Vocabulary into a set\n",
    "  vocabulary = set(vocabulary)\n",
    "\n",
    "  # Create empty list for sentences\n",
    "  new_tokenized_sentences = []\n",
    "  \n",
    "  # Iterate over sentences\n",
    "  for sentence in tokenized_sentences:\n",
    "\n",
    "    # Iterate over sentence and add <unk> \n",
    "    # if the token is absent from the vocabulary\n",
    "    new_sentence = []\n",
    "    for token in sentence:\n",
    "      if token in vocabulary:\n",
    "        new_sentence.append(token)\n",
    "      else:\n",
    "        new_sentence.append(unknown_token)\n",
    "    \n",
    "    # Append sentece to the new list\n",
    "    new_tokenized_sentences.append(new_sentence)\n",
    "\n",
    "  return new_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:14:09.191716Z",
     "iopub.status.busy": "2021-10-12T07:14:09.191485Z",
     "iopub.status.idle": "2021-10-12T07:14:09.203457Z",
     "shell.execute_reply": "2021-10-12T07:14:09.202473Z",
     "shell.execute_reply.started": "2021-10-12T07:14:09.191690Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleansing(train_data, test_data, count_threshold):\n",
    "    \n",
    "  # Get closed Vocabulary\n",
    "  vocabulary = handling_oov(train_data, count_threshold)\n",
    "    \n",
    "  # Updated Training Dataset\n",
    "  new_train_data = unk_tokenize(train_data, vocabulary)\n",
    "    \n",
    "  # Updated Test Dataset\n",
    "  new_test_data = unk_tokenize(test_data, vocabulary)\n",
    "\n",
    "  return new_train_data, new_test_data, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:14:09.204893Z",
     "iopub.status.busy": "2021-10-12T07:14:09.204649Z",
     "iopub.status.idle": "2021-10-12T07:14:40.499371Z",
     "shell.execute_reply": "2021-10-12T07:14:40.498496Z",
     "shell.execute_reply.started": "2021-10-12T07:14:09.204865Z"
    }
   },
   "outputs": [],
   "source": [
    "min_freq = 4\n",
    "final_train, final_test, vocabulary = cleansing(train, test, min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:14:40.502437Z",
     "iopub.status.busy": "2021-10-12T07:14:40.501634Z",
     "iopub.status.idle": "2021-10-12T07:14:40.511400Z",
     "shell.execute_reply": "2021-10-12T07:14:40.509776Z",
     "shell.execute_reply.started": "2021-10-12T07:14:40.502396Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
    "\n",
    "  # Empty dict for n-grams\n",
    "  n_grams = {}\n",
    " \n",
    "  # Iterate over all sentences in the dataset\n",
    "  for sentence in data:\n",
    "        \n",
    "    # Append n start tokens and a single end token to the sentence\n",
    "    sentence = [start_token]*n + sentence + [end_token]\n",
    "    \n",
    "    # Convert the sentence into a tuple\n",
    "    sentence = tuple(sentence)\n",
    "\n",
    "    # Temp var to store length from start of n-gram to end\n",
    "    m = len(sentence) if n==1 else len(sentence)-1\n",
    "    \n",
    "    # Iterate over this length\n",
    "    for i in range(m):\n",
    "        \n",
    "      # Get the n-gram\n",
    "      n_gram = sentence[i:i+n]\n",
    "    \n",
    "      # Add the count of n-gram as value to our dictionary\n",
    "      # IF n-gram is already present\n",
    "      if n_gram in n_grams.keys():\n",
    "        n_grams[n_gram] += 1\n",
    "      # Add n-gram count\n",
    "      else:\n",
    "        n_grams[n_gram] = 1\n",
    "        \n",
    "  return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:14:40.513015Z",
     "iopub.status.busy": "2021-10-12T07:14:40.512734Z",
     "iopub.status.idle": "2021-10-12T07:14:40.527564Z",
     "shell.execute_reply": "2021-10-12T07:14:40.526438Z",
     "shell.execute_reply.started": "2021-10-12T07:14:40.512973Z"
    }
   },
   "outputs": [],
   "source": [
    "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
    "\n",
    "  # Convert the previous_n_gram into a tuple \n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
    "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "  \n",
    "  # The Denominator\n",
    "  denom = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "  # previous n-gram plus the current word as a tuple\n",
    "  nplus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n",
    "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
    "\n",
    "  # Numerator\n",
    "  num = nplus1_gram_count + k\n",
    "\n",
    "  # Final Fraction\n",
    "  prob = num / denom\n",
    "  return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:29:37.354329Z",
     "iopub.status.busy": "2021-10-12T07:29:37.353431Z",
     "iopub.status.idle": "2021-10-12T07:29:37.360788Z",
     "shell.execute_reply": "2021-10-12T07:29:37.360128Z",
     "shell.execute_reply.started": "2021-10-12T07:29:37.354292Z"
    }
   },
   "outputs": [],
   "source": [
    "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
    "\n",
    "  # Convert to Tuple\n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "  # Add end and unknown tokens to the vocabulary\n",
    "  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "\n",
    "  # Calculate the size of the vocabulary\n",
    "  vocabulary_size = len(vocabulary)\n",
    "\n",
    "  # Empty dict for probabilites\n",
    "  probabilities = {}\n",
    "\n",
    "  # Iterate over words \n",
    "  for word in vocabulary:\n",
    "    \n",
    "    # Calculate probability\n",
    "    probability = prob_for_single_word(word, previous_n_gram, \n",
    "                                           n_gram_counts, nplus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "    # Create mapping: word -> probability\n",
    "    probabilities[word] = probability\n",
    "\n",
    "  return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:29:42.468199Z",
     "iopub.status.busy": "2021-10-12T07:29:42.467845Z",
     "iopub.status.idle": "2021-10-12T07:29:42.475421Z",
     "shell.execute_reply": "2021-10-12T07:29:42.474477Z",
     "shell.execute_reply.started": "2021-10-12T07:29:42.468170Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # most recent 'n' words\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    # Calculate probabilty for all words\n",
    "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
    "\n",
    "    # Intialize the suggestion and max probability\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "\n",
    "    # Iterate over all words and probabilites, returning the max.\n",
    "    # We also add a check if the start_with parameter is provided\n",
    "    for word, prob in probabilities.items():\n",
    "        \n",
    "        if start_with != None: \n",
    "            \n",
    "            if not word.startswith(start_with):\n",
    "                continue \n",
    "\n",
    "        if prob > max_prob: \n",
    "\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:29:52.413030Z",
     "iopub.status.busy": "2021-10-12T07:29:52.412404Z",
     "iopub.status.idle": "2021-10-12T07:32:34.080828Z",
     "shell.execute_reply": "2021-10-12T07:32:34.079245Z",
     "shell.execute_reply.started": "2021-10-12T07:29:52.412989Z"
    }
   },
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    n_model_counts = count_n_grams(final_train, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:33:20.413444Z",
     "iopub.status.busy": "2021-10-12T07:33:20.412930Z",
     "iopub.status.idle": "2021-10-12T07:33:21.923067Z",
     "shell.execute_reply": "2021-10-12T07:33:21.922070Z",
     "shell.execute_reply.started": "2021-10-12T07:33:20.413410Z"
    }
   },
   "outputs": [],
   "source": [
    "previous_tokens = [\"i\", \"was\", \"about\"]\n",
    "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "display(suggestion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
